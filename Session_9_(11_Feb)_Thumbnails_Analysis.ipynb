{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Pf_7IeiqiaLmu_EPTkwc81VAD7CP3NtJ",
      "authorship_tag": "ABX9TyPUWiGdeEhtQxTUeFvHPAUs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Miit-009/Jio-Institute/blob/main/Session_9_(11_Feb)_Thumbnails_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHClzbAerRJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa1473c9-be6c-4bbe-9cf6-a89d654569cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting face-recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.11/dist-packages (19.24.2)\n",
            "Collecting face-recognition-models>=0.3.0 (from face-recognition)\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.11/dist-packages (from face-recognition) (8.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from face-recognition) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from face-recognition) (11.1.0)\n",
            "Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566162 sha256=45d6b8a37df0a9d69b83cf3b77d81a8fec4e7b63fea66bfd1afcd1a0c16914b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/52/ec/9355da79c29f160b038a20c784db2803c2f9fa2c8a462c176a\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install face-recognition dlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from skimage import exposure\n",
        "from skimage.color import rgb2hsv\n",
        "from skimage.filters import gaussian\n",
        "from skimage.exposure import is_low_contrast\n",
        "from skimage.measure import shannon_entropy\n",
        "from PIL import Image\n",
        "\n",
        "# Load OpenCV's Haar cascade face detector\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "def analyze_image(image_path):\n",
        "    image_path = Path(image_path)  # Convert string path to Path object\n",
        "    image = cv2.imread(str(image_path))\n",
        "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces using OpenCV\n",
        "    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    # Face details\n",
        "    face_present = len(faces) > 0\n",
        "    num_faces = len(faces)\n",
        "    face_size_ratios = [(w * h) / (image.shape[0] * image.shape[1]) for (x, y, w, h) in faces]\n",
        "\n",
        "    # Brightness\n",
        "    brightness = np.mean(gray_image)\n",
        "\n",
        "    # Contrast\n",
        "    contrast = gray_image.std()\n",
        "\n",
        "    # Hue, Saturation (HSV conversion)\n",
        "    hsv_image = rgb2hsv(rgb_image)\n",
        "    hue = np.mean(hsv_image[:, :, 0])\n",
        "    saturation = np.mean(hsv_image[:, :, 1])\n",
        "\n",
        "    # Sharpness\n",
        "    laplacian_var = cv2.Laplacian(gray_image, cv2.CV_64F).var()\n",
        "\n",
        "    # White Balance Deviation\n",
        "    mean_r, mean_g, mean_b = np.mean(rgb_image, axis=(0, 1))\n",
        "    white_balance_deviation = max(abs(mean_r - mean_g), abs(mean_g - mean_b), abs(mean_b - mean_r))\n",
        "\n",
        "    # Exposure\n",
        "    exposure_value = np.mean(rgb_image) / 255.0\n",
        "\n",
        "    # Aspect Ratio\n",
        "    height, width = image.shape[:2]\n",
        "    aspect_ratio = width / height\n",
        "    resolution = width * height\n",
        "\n",
        "    # Gamma Estimation\n",
        "    estimated_gamma = exposure.adjust_gamma(image, gamma=1).mean() / 255.0\n",
        "\n",
        "    # Check for low contrast\n",
        "    low_contrast = is_low_contrast(gray_image)\n",
        "\n",
        "    return {\n",
        "        \"video_id\": image_path.stem,\n",
        "        \"face_present\": face_present,\n",
        "        \"num_faces\": num_faces,\n",
        "        \"face_size_ratios\": face_size_ratios if face_size_ratios else None,\n",
        "        \"dominant_emotions\": \"N/A\",  # Requires an external emotion detection library\n",
        "        \"width\": width,\n",
        "        \"height\": height,\n",
        "        \"resolution\": resolution,\n",
        "        \"aspect_ratio\": aspect_ratio,\n",
        "        \"brightness\": brightness,\n",
        "        \"contrast\": contrast,\n",
        "        \"saturation\": saturation,\n",
        "        \"hue\": hue,\n",
        "        \"estimated_gamma\": estimated_gamma,\n",
        "        \"sharpness\": laplacian_var,\n",
        "        \"white_balance_deviation\": white_balance_deviation,\n",
        "        \"exposure\": exposure_value\n",
        "    }\n",
        "\n",
        "def process_images_colab(output_excel):\n",
        "    from google.colab import files\n",
        "    import os\n",
        "\n",
        "    folder_path = \"/content\"\n",
        "    image_files = [f for f in os.listdir(folder_path) if f.endswith(\".jpg\")]\n",
        "    data = [analyze_image(os.path.join(folder_path, image)) for image in image_files]\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_excel(output_excel, index=False)\n",
        "    files.download(output_excel)\n",
        "    print(f\"Excel file saved and downloaded: {output_excel}\")\n",
        "\n",
        "# Usage\n",
        "process_images_colab(\"video_analysis.xlsx\")\n"
      ],
      "metadata": {
        "id": "ok5G_bIbuvG-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}